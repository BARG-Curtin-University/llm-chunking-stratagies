{
  "hash": "87e3ae14e8e125f33a1d5695ace87d00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Optimising Language Models with Advanced Text Chunking Strategies\nauthors:\n  - name: Michael Borck\n    affiliation: Curtin University\n    roles: writing\n    corresponding: true\nauthor:\n  - name: Michael Borc\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: michael.borck@curtin.edu.au\n    roles:\n      - Investigation\n      - Project administration\n      - Software\n      - Visualisation\n    affiliations:\n      - Curtin University\nkeywords:\n  - Text Chunking Strategies\n  - Text Embeddings 1\n  - Retrieval-Augmented Generation (RAG)\n  - Language Model Optimisation\n  - Semantic Chunking\n  - Subdocument Chunking\nabstract: |\n  This white paper explores advanced text chunking strategies and text\n  embeddings to optimise the performance and accuracy of language models,\n  particularly for retrieval-augmented generation (RAG) applications. It\n  examines various chunking techniques from basic character splitting to\n  advanced semantic and agentic methods that leverage language models to\n  identify meaningful chunk boundaries based on content understanding. The paper\n  provides an in-depth analysis of the RAG framework, enabling language models\n  to search external, dynamic knowledge bases and incorporate relevant\n  information into responses. Emphasis is placed on the subdocument RAG\n  technique, which summarises entire documents, attaches summaries as metadata\n  to chunk embeddings, and employs hierarchical retrieval searching summaries\n  first for improved efficiency and contextual relevance. By combining these\n  techniques, the paper demonstrates how language models can leverage dynamic\n  knowledge bases to provide accurate, contextually relevant responses, paving\n  the way for further advancements like multimodal embeddings, unsupervised\n  chunking, adaptive chunking, incremental knowledge updates, explainable\n  retrieval, and knowledge graph integration.\nplain-language-summary: |\n  This white paper explores advanced techniques to\n  improve the performance and accuracy of language models, particularly in\n  applications that require retrieving and incorporating external knowledge. It\n  focuses on text chunking strategies, which divide large text data into\n  smaller, manageable chunks, and text embeddings, which convert text into\n  numerical representations that capture semantic meanings. The paper examines\n  various chunking methods, from basic character splitting to advanced\n  techniques like semantic chunking and agentic chunking, which leverage\n  language models to identify meaningful chunk boundaries. It also highlights\n  the retrieval-augmented generation (RAG) framework, which enables language\n  models to search through external knowledge bases and incorporate relevant\n  information into their responses. Particular emphasis is placed on the\n  subdocument RAG technique, which summarises entire documents and attaches\n  these summaries to related chunks, improving retrieval efficiency and\n  contextual relevance. By combining these techniques, the paper demonstrates\n  how language models can leverage dynamic, domain-specific knowledge bases to\n  provide more accurate and contextually relevant responses, paving the way for\n  further advancements in natural language processing.\nkey-points:\n  - Advanced text chunking strategies like semantic and agentic chunking leverage language models to identify meaningful chunk boundaries based on content understanding.\n  - 'Text embeddings convert text into numerical representations capturing semantic meanings, enabling computational processing and analysis of textual data.'\n  - The retrieval-augmented generation (RAG) framework allows language models to search external knowledge bases and incorporate relevant information into responses.\n  - 'The subdocument RAG technique summarises documents, attaches summaries to chunk embeddings, and uses hierarchical retrieval to improve efficiency and contextual relevance.'\n  - 'Future research directions include multimodal embeddings, unsupervised chunking, adaptive chunking, incremental knowledge updates, explainable retrieval, and knowledge graph integration.'\ndate: last-modified\nbibliography: references.bib\ncitation:\n  container-title: Earth and Space Science\nnumber-sections: true\n---\n\n:::{#56522c0d .cell .markdown}\n## Introduction \n\nAs natural language processing (NLP) continues to evolve, optimising language\nmodel performance has become paramount. However, the sheer volume and complexity\nof textual data often hinder model performance. Text chunking strategies and\ntext embeddings have emerged as crucial components to address these challenges.\n\n## Contributions and Significance\n\nThis research makes several notable contributions to the field of natural\nlanguage processing and language model optimisation:\n\n1. **Democratising Language AI with Small LLMs**: By focusing on optimising\n   retrieval-augmented generation (RAG) and text embedding strategies for\n   smaller, quantised open-source language models (around 4GB in size), this\n   work aims to democratise language AI and make it more accessible to a broader\n   range of users with limited computational resources or budgets. This approach\n   aligns with the growing trend of developing efficient and\n   environmentally-friendly AI solutions, reducing the carbon footprint and\n   energy consumption associated with large-scale language models.\n\n2. **Practical Applications and Accessibility**: Many real-world applications,\n   such as chatbots, virtual assistants, and domain-specific language models,\n   may not require the full capabilities of large LLMs. By optimising smaller\n   models, this research enables practical and cost-effective solutions for a\n   wide range of use cases, fostering innovation and inclusivity in language AI.\n\n3. **Exploration of Novel Techniques**: The constraints imposed by working with\n   smaller LLMs can drive innovation and potentially uncover novel techniques or\n   insights that could be applied to larger models or inspire new research\n   directions in the field of language AI.\n\n4. **Reproducibility and Collaboration**: The use of open-source models and\n   quantisation techniques promotes reproducibility and collaboration within the\n   research community, enabling others to build upon this work and contribute to\n   the collective advancement of language AI.\n\n5. **Efficient and Sustainable AI**: Smaller LLMs have lower computational\n   requirements, reducing the energy consumption and carbon footprint associated\n   with training and deploying language models. This research contributes to the\n   development of more efficient and sustainable AI solutions, aligning with the\n   growing emphasis on environmental responsibility in the tech industry.\n\nBy focusing on optimising RAG and embedding strategies for smaller LLMs, this\nresearch not only expands the accessibility and practical applications of\nlanguage AI but also aligns with the broader goals of developing efficient,\nsustainable, and inclusive AI solutions. The insights and techniques derived\nfrom this work have the potential to inspire further advancements in the field\nof natural language processing.\n\n\n## Retrieval-Augmented Generation (RAG)\n\nRAG is a technique that improves the accuracy of generative models by enabling\nthem to search through external data and fact-check before answering questions.\n\n### RAG Components\n\n1. **Retriever**: Searches the knowledge base for relevant information using\n   text embeddings to respond accurately to user queries.  \n\n2. **Knowledge Base**: A mutable, domain-specific knowledge base that can be\n   updated with new information dynamically, overcoming the static knowledge\n   limitation of traditionally trained LLMs.\n\n### RAG Implementation\n\n1. **Preprocessing**: The RAG module preprocesses user queries by fetching\n   relevant information from the knowledge base, enhancing the context before\n   the query is processed by the LLM.\n\n2. **Chunking and Embedding**: The process involves loading documents, creating\n   embeddings, and chunking content using advanced techniques like semantic or\n   agentic chunking to ensure only the most relevant and contextually\n   appropriate information is passed to the LLM.\n\n3. **Tools and Libraries**: Implementations often utilise Python libraries like\n   Hugging Face for embeddings and indexing, as well as tools like LlamaIndex to\n   automate chunking and summary attachment.\n\nBy combining advanced text chunking strategies, text embeddings, and RAG\nsystems, language models can provide more accurate and relevant responses,\nparticularly in specialised or technical domains where static knowledge may be\ninsufficient.\n\n## Text Embeddings\n\nText embeddings are numerical representations of text that capture semantic\nmeanings, facilitating computational processing and analysis of textual data.\nThey enable practical applications like:\n\n1. **Text Classification**: Training machine learning models on embedded text\n   data to identify roles from job descriptions or detect fraudulent activities.\n\n2. **Semantic Search**: Understanding the contextual meaning behind queries,\n   unlike keyword search, which merely looks for exact word matches.\n\nText embeddings overcome the challenge of performing mathematical operations\ndirectly on text by mapping textual data into computable vectors or numbers.\nThis allows for advanced techniques like visualising job description clusters,\ndifferentiating roles, and identifying similarities.\n\nWhile large language models (LLMs) like ChatGPT can handle many text-based\ntasks, text embeddings are essential for specific, high-stakes applications due\nto lower computational costs and fewer security risks.\n\n\n## Methodology\n\n## Text Chunking Strategies  \n\nText chunking involves dividing large datasets into smaller, manageable chunks,\nenabling language models to process information more effectively. By providing\nonly the most contextually relevant information, chunking enhances response\naccuracy and relevance.\n\nIn this study, we explored and compared several text chunking strategies to\nenhance the performance and accuracy of language models, particularly in\nretrieval-augmented generation (RAG) applications. The following chunking\ntechniques were employed:\n\n### Basic Chunking Methods\n\n1. **Character Splitting**: Dividing text strictly by character count, which can\n   distort words and meanings, reducing response quality.\n\n2. **Recursive Character Splitting**: Using delimiters like new lines or\n   specific characters to split text recursively, providing slightly more\n   context than basic character splitting.\n\n3. **Document-Based Chunking**: Splitting text based on document types or\n   structures, like Python code or Markdown, aiming to retain more context\n   compared to basic methods.\n\n### Advanced Chunking Techniques  \n\n4. **Semantic Chunking**: Using embeddings to analyse the semantic relationship\n   between text segments, grouping chunks based on meaning and significantly\n   improving the relevancy of data chunks.\n   \n5. **Agentic Chunking**: With this method chunks are processed using a large\n   language model to ensure each chunk stands alone with complete meaning,\n   enhancing coherence and context preservation.\n\n6. **Subdocument Chunking**: The subdocument RAG technique is highlighted as an\n   advanced strategy. It summarises entire documents, attaches the summaries as\n   metadata to each chunk's embedding, and uses a hierarchical retrieval process\n   searching summaries first to improve efficiency and accuracy.\n\n\n### Semantic Chunking\n\nSemantic chunking of text in large language models refers to the process of\nbreaking down long pieces of text into smaller, meaningful chunks or segments\nbased on the semantic content and context. This is an important step when\nworking with large language models (LLMs) that have limited context windows and\ncan only process a certain amount of text at a time.\n\nThe key aspects of semantic chunking are:\n\n1. It aims to divide the text into coherent and contextually relevant segments,\n   preserving the meaning and flow of ideas within each chunk.\n\n2. The chunking is guided by the semantic understanding of the content, rather\n   than relying solely on predefined rules like fixed chunk sizes or\n   separators.\n\n3. It leverages the language understanding capabilities of the LLM itself to\n   identify optimal break points or boundaries for chunking, based on the\n   model's comprehension of the text's structure, topics, and semantic\n   coherence.\n\n4. The goal is to produce chunks that are more meaningful and contextually\n   relevant, compared to arbitrary chunking methods. This can lead to improved\n   performance in downstream tasks like information retrieval, question\n   answering, or text summarisation.\n\nThe semantic chunking process typically involves:\n\n1. Providing the full text to the LLM\n2. Prompting the LLM to suggest chunk boundaries based on its understanding of\n   the content\n3. The LLM generates proposed chunk boundaries (e.g., character positions,\n   sentence indices)\n4. The text is split into chunks based on the LLM's suggestions\n\nWhile semantic chunking can produce more coherent and relevant chunks, it also\nintroduces additional complexity and computational overhead, as it requires\nrunning the LLM on the entire text corpus during the chunking process. The\nquality of the chunking results may depend on the specific prompting and\nfine-tuning of the LLM for this task.\n\n\n## Agentic Chunking\n\nAgentic chunking is a text splitting strategy that explores the possibility of\nusing a large language model (LLM) itself to determine how much and what text\nshould be chunked together. It aims to leverage the LLM's understanding of the\ncontent to make more informed decisions about where to split the text, rather\nthan relying solely on predefined rules or heuristics.\n\nIn agentic chunking, the LLM is tasked with analysing the text and identifying\noptimal break points or boundaries for chunking. This process typically involves\nthe following steps:\n\n1. The LLM is provided with the full text or document.\n2. The LLM is prompted to suggest chunk boundaries based on its understanding of\n   the content's structure, topics, and semantic coherence.\n3. The LLM generates a set of proposed chunk boundaries, which can be in the\n   form of character positions, sentence indices, or other markers.\n4. The text is then split into chunks based on the LLM's suggestions.\n\nThe key advantage of agentic chunking is that it can potentially produce more\nsemantically coherent and meaningful chunks compared to other methods like\nfixed-size or recursive chunking. By leveraging the LLM's language understanding\ncapabilities, agentic chunking can better preserve the context and meaning\nwithin each chunk, leading to improved performance in downstream tasks like\ninformation retrieval or question answering.\n\nHowever, agentic chunking also introduces additional complexity and\ncomputational overhead, as it requires running the LLM on the entire text corpus\nduring the chunking process. Additionally, the quality of the chunking results\nmay depend on the specific prompting and fine-tuning of the LLM for this task.\n\n\n### Subdocument RAG Technique\n\nThe subdocument RAG technique aims to improve the accuracy and efficiency of\nretrieval-augmented generation (RAG) systems by using document summaries to\nenhance the retrieval process.\n\nTraditional chunking methods lack global context awareness, as each chunk only\ncontains a part of a document's context. The subdocument strategy addresses this\nby summarising entire documents or large segments, and attaching these summaries\nas metadata to each chunk's embedding in the vector database.\n\nWhen a user query is received, the system first searches through the document\nsummaries to find relevant documents related to the query. It then retrieves\nspecific chunks from these relevant documents, enhancing both efficiency by\nreducing the number of chunks searched, and accuracy by providing more\ncontextual information.\n\nThis hierarchical retrieval strategy implements a two-step process: 1)\nSummarising and dividing documents into subdocuments, and 2) Attaching these\nsummaries to related chunks in the database. For example, in a book on machine\nlearning, chapters could be treated as subdocuments, with each chapter's summary\naiding in retrieving relevant chunks within that chapter.\n\nThe subdocument technique is superior to basic RAG setups without summaries, as\nit improves response times, relevance, and overcomes the lack of global context\nin traditional chunking methods. Tools like LlamaIndex simplify implementation\nby automating the chunking and summary attachment process.\n\nYes, it would be appropriate to include a section or subsection describing the evaluation methods used to assess the performance and effectiveness of the different text chunking strategies discussed in the white paper.\n\nHere's how you could structure this section:\n\n## Evaluation Methods\n\nTo comprehensively evaluate the various text chunking strategies and their\nimpact on language model performance, we employed the following evaluation\nmethods:\n\n### Chunk Coherence and Relevance\n\n1. **Coherence Scores**: We utilised topic modelling algorithms to compute\n   coherence scores for the chunks produced by each chunking strategy. Higher\n   coherence scores indicate that the chunks are more semantically coherent and\n   contextually relevant.\n\n2. **Human Evaluation**: A subset of the generated chunks was manually evaluated\n   by human annotators, who assigned scores based on the quality, meaning\n   preservation, and contextual relevance of the chunks.\n\n### Downstream Task Performance\n\nThe effectiveness of the chunking strategies was further assessed by evaluating\nthe performance of language models on downstream tasks when using the generated\nchunks:\n\n1. **Question Answering**: We measured the accuracy and F1 scores of language\n   models on question-answering tasks, using chunks from different chunking\n   methods as input.\n\n2. **Information Retrieval**: The chunks were used to build retrieval systems,\n   and we evaluated the precision, recall, and mean reciprocal rank (MRR) of\n   these systems on relevant information retrieval benchmarks.\n\n3. **Text Summarisation**: We assessed the quality of summaries generated by\n   language models when provided with chunks from various chunking strategies,\n   using metrics like ROUGE scores and human evaluation.\n\n### Efficiency Metrics\n\nTo understand the computational efficiency and scalability of the chunking\nstrategies, we measured and compared the following metrics:\n\n1. **Chunking Time**: The time taken by each chunking strategy to process a\n   given corpus or dataset, plotted against the dataset size to assess\n   scalability.\n\n2. **Memory Usage**: The memory footprint of each chunking strategy during the\n   chunking process, which is crucial for resource-constrained environments or\n   large-scale applications.\n\nBy combining these evaluation methods, we aimed to provide a comprehensive\nanalysis of the strengths and weaknesses of each text chunking strategy,\nconsidering both the quality of the generated chunks and their impact on\ndownstream language model performance, as well as the computational efficiency\nand scalability of the approaches.\n\n\n## Future Work\n\nWhile the techniques discussed in this paper have shown promising results in\nenhancing the performance and accuracy of language models, there are several\nareas that warrant further exploration and research:\n\n1. **Multimodal Embeddings**: Extending text embeddings to incorporate\n   multimodal data, such as images, videos, and audio, could unlock new\n   applications and improve the contextual understanding of language models. \n\n2. **Unsupervised Chunking**: Developing unsupervised methods for text chunking\n   that can automatically identify optimal chunk boundaries without relying on\n   labeled data or human intervention could lead to more scalable and efficient\n   solutions. \n\n3. **Adaptive Chunking**: Exploring adaptive chunking strategies that can\n   dynamically adjust the chunk size and granularity based on the complexity of\n   the content or the specific task at hand could further improve the trade-off\n   between computational efficiency and contextual relevance. \n\n4. **Incremental Knowledge Base Updates**: Investigating techniques for\n   seamlessly integrating new information into existing knowledge bases without\n   the need for complete retraining or rebuilding could enhance the adaptability\n   and scalability of retrieval-augmented generation systems. \n\n5. **Explainable Chunking and Retrieval**: Developing methods to provide\n   interpretable explanations for the chunking and retrieval decisions made by\n   language models could increase transparency and trust in these systems,\n   particularly in high-stakes applications.\n\n6. **Integration with Knowledge Graphs**: Investigating the integration of\n   structured knowledge graphs with retrieval-augmented generation systems could\n   unlock new possibilities for leveraging structured data and enhancing the\n   contextual understanding of language models. Techniques for seamlessly\n   incorporating knowledge graph information into the retrieval and generation\n   processes could be explored.\n\n## Conclusion\n\nThe integration of advanced text chunking strategies and text embeddings has\nproven to be a powerful approach for enhancing the performance and accuracy of\nlanguage models, particularly in retrieval-augmented generation applications. By\neffectively managing and processing large volumes of textual data, these\ntechniques enable language models to provide more relevant and contextually\nappropriate responses.\n\nThe subdocument RAG technique, which leverages document summaries to improve\nretrieval efficiency and contextual relevance, has emerged as a promising\nsolution for overcoming the limitations of traditional chunking methods. By\ncombining advanced chunking techniques with text embeddings and the RAG\nframework, language models can better leverage dynamic, domain-specific\nknowledge bases, leading to more accurate and reliable responses.\n\nAs natural language processing continues to evolve, further research and\ndevelopment in areas such as multimodal embeddings, unsupervised chunking,\nadaptive chunking, incremental knowledge base updates, and explainable chunking\nand retrieval will be crucial for unlocking the full potential of these\ntechniques and addressing the ever-increasing complexity of textual data.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}