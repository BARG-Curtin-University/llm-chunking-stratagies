<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michael Borck">
<meta name="dcterms.date" content="2024-04-24">
<meta name="keywords" content="Text Chunking Strategies, Text Embeddings 1, Retrieval-Augmented Generation (RAG), Language Model Optimisation, Semantic Chunking, Subdocument Chunking">

<title>Optimising Language Models with Advanced Text Chunking Strategies</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<meta name="citation_title" content="Optimising Language Models with Advanced Text Chunking Strategies">
<meta name="citation_abstract" content="This white paper explores advanced text chunking strategies and text
embeddings to optimise the performance and accuracy of language models,
particularly for retrieval-augmented generation (RAG) applications. It
examines various chunking techniques from basic character splitting to
advanced semantic and agentic methods that leverage language models to
identify meaningful chunk boundaries based on content understanding. The paper
provides an in-depth analysis of the RAG framework, enabling language models
to search external, dynamic knowledge bases and incorporate relevant
information into responses. Emphasis is placed on the subdocument RAG
technique, which summarises entire documents, attaches summaries as metadata
to chunk embeddings, and employs hierarchical retrieval searching summaries
first for improved efficiency and contextual relevance. By combining these
techniques, the paper demonstrates how language models can leverage dynamic
knowledge bases to provide accurate, contextually relevant responses, paving
the way for further advancements like multimodal embeddings, unsupervised
chunking, adaptive chunking, incremental knowledge updates, explainable
retrieval, and knowledge graph integration.
">
<meta name="citation_keywords" content="Text Chunking Strategies,Text Embeddings 1,Retrieval-Augmented Generation (RAG),Language Model Optimisation,Semantic Chunking,Subdocument Chunking">
<meta name="citation_author" content="Michael Borck">
<meta name="citation_publication_date" content="2024-04-24">
<meta name="citation_cover_date" content="2024-04-24">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-04-24">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="BARG Curtin University">
<meta name="citation_reference" content="citation_title=Literate programming;,citation_author=Donald E. Knuth;,citation_publication_date=1984-05;,citation_cover_date=1984-05;,citation_year=1984;,citation_fulltext_html_url=https://doi.org/10.1093/comjnl/27.2.97;,citation_issue=2;,citation_doi=10.1093/comjnl/27.2.97;,citation_issn=0010-4620;,citation_volume=27;,citation_journal_title=Comput. J.;,citation_publisher=Oxford University Press, Inc.;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Optimising Language Models with Advanced Text Chunking Strategies</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliation</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Michael Borck <a href="mailto:michael.borck@curtin.edu.au" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-0950-6396" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Curtin University
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">April 24, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This white paper explores advanced text chunking strategies and text embeddings to optimise the performance and accuracy of language models, particularly for retrieval-augmented generation (RAG) applications. It examines various chunking techniques from basic character splitting to advanced semantic and agentic methods that leverage language models to identify meaningful chunk boundaries based on content understanding. The paper provides an in-depth analysis of the RAG framework, enabling language models to search external, dynamic knowledge bases and incorporate relevant information into responses. Emphasis is placed on the subdocument RAG technique, which summarises entire documents, attaches summaries as metadata to chunk embeddings, and employs hierarchical retrieval searching summaries first for improved efficiency and contextual relevance. By combining these techniques, the paper demonstrates how language models can leverage dynamic knowledge bases to provide accurate, contextually relevant responses, paving the way for further advancements like multimodal embeddings, unsupervised chunking, adaptive chunking, incremental knowledge updates, explainable retrieval, and knowledge graph integration.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>Text Chunking Strategies, Text Embeddings 1, Retrieval-Augmented Generation (RAG), Language Model Optimisation, Semantic Chunking, Subdocument Chunking</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#contributions-and-significance" id="toc-contributions-and-significance" class="nav-link" data-scroll-target="#contributions-and-significance"><span class="header-section-number">2</span> Contributions and Significance</a></li>
  <li><a href="#retrieval-augmented-generation-rag" id="toc-retrieval-augmented-generation-rag" class="nav-link" data-scroll-target="#retrieval-augmented-generation-rag"><span class="header-section-number">3</span> Retrieval-Augmented Generation (RAG)</a>
  <ul class="collapse">
  <li><a href="#rag-components" id="toc-rag-components" class="nav-link" data-scroll-target="#rag-components"><span class="header-section-number">3.1</span> RAG Components</a></li>
  <li><a href="#rag-implementation" id="toc-rag-implementation" class="nav-link" data-scroll-target="#rag-implementation"><span class="header-section-number">3.2</span> RAG Implementation</a></li>
  </ul></li>
  <li><a href="#text-embeddings" id="toc-text-embeddings" class="nav-link" data-scroll-target="#text-embeddings"><span class="header-section-number">4</span> Text Embeddings</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology"><span class="header-section-number">5</span> Methodology</a></li>
  <li><a href="#text-chunking-strategies" id="toc-text-chunking-strategies" class="nav-link" data-scroll-target="#text-chunking-strategies"><span class="header-section-number">6</span> Text Chunking Strategies</a>
  <ul class="collapse">
  <li><a href="#basic-chunking-methods" id="toc-basic-chunking-methods" class="nav-link" data-scroll-target="#basic-chunking-methods"><span class="header-section-number">6.1</span> Basic Chunking Methods</a></li>
  <li><a href="#advanced-chunking-techniques" id="toc-advanced-chunking-techniques" class="nav-link" data-scroll-target="#advanced-chunking-techniques"><span class="header-section-number">6.2</span> Advanced Chunking Techniques</a></li>
  <li><a href="#semantic-chunking" id="toc-semantic-chunking" class="nav-link" data-scroll-target="#semantic-chunking"><span class="header-section-number">6.3</span> Semantic Chunking</a></li>
  </ul></li>
  <li><a href="#agentic-chunking" id="toc-agentic-chunking" class="nav-link" data-scroll-target="#agentic-chunking"><span class="header-section-number">7</span> Agentic Chunking</a>
  <ul class="collapse">
  <li><a href="#subdocument-rag-technique" id="toc-subdocument-rag-technique" class="nav-link" data-scroll-target="#subdocument-rag-technique"><span class="header-section-number">7.1</span> Subdocument RAG Technique</a></li>
  </ul></li>
  <li><a href="#evaluation-methods" id="toc-evaluation-methods" class="nav-link" data-scroll-target="#evaluation-methods"><span class="header-section-number">8</span> Evaluation Methods</a>
  <ul class="collapse">
  <li><a href="#chunk-coherence-and-relevance" id="toc-chunk-coherence-and-relevance" class="nav-link" data-scroll-target="#chunk-coherence-and-relevance"><span class="header-section-number">8.1</span> Chunk Coherence and Relevance</a></li>
  <li><a href="#downstream-task-performance" id="toc-downstream-task-performance" class="nav-link" data-scroll-target="#downstream-task-performance"><span class="header-section-number">8.2</span> Downstream Task Performance</a></li>
  <li><a href="#efficiency-metrics" id="toc-efficiency-metrics" class="nav-link" data-scroll-target="#efficiency-metrics"><span class="header-section-number">8.3</span> Efficiency Metrics</a></li>
  </ul></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work"><span class="header-section-number">9</span> Future Work</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">10</span> Conclusion</a></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="src/subdoc-summary-preview.html"><i class="bi bi-journal-code"></i>Sub-Document Summary Metadata Pack</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>As natural language processing (NLP) continues to evolve, optimising language model performance has become paramount. However, the sheer volume and complexity of textual data often hinder model performance. Text chunking strategies and text embeddings have emerged as crucial components to address these challenges.</p>
</section>
<section id="contributions-and-significance" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="contributions-and-significance"><span class="header-section-number">2</span> Contributions and Significance</h2>
<p>This research makes several notable contributions to the field of natural language processing and language model optimisation:</p>
<ol type="1">
<li><p><strong>Democratising Language AI with Small LLMs</strong>: By focusing on optimising retrieval-augmented generation (RAG) and text embedding strategies for smaller, quantised open-source language models (around 4GB in size), this work aims to democratise language AI and make it more accessible to a broader range of users with limited computational resources or budgets. This approach aligns with the growing trend of developing efficient and environmentally-friendly AI solutions, reducing the carbon footprint and energy consumption associated with large-scale language models.</p></li>
<li><p><strong>Practical Applications and Accessibility</strong>: Many real-world applications, such as chatbots, virtual assistants, and domain-specific language models, may not require the full capabilities of large LLMs. By optimising smaller models, this research enables practical and cost-effective solutions for a wide range of use cases, fostering innovation and inclusivity in language AI.</p></li>
<li><p><strong>Exploration of Novel Techniques</strong>: The constraints imposed by working with smaller LLMs can drive innovation and potentially uncover novel techniques or insights that could be applied to larger models or inspire new research directions in the field of language AI.</p></li>
<li><p><strong>Reproducibility and Collaboration</strong>: The use of open-source models and quantisation techniques promotes reproducibility and collaboration within the research community, enabling others to build upon this work and contribute to the collective advancement of language AI.</p></li>
<li><p><strong>Efficient and Sustainable AI</strong>: Smaller LLMs have lower computational requirements, reducing the energy consumption and carbon footprint associated with training and deploying language models. This research contributes to the development of more efficient and sustainable AI solutions, aligning with the growing emphasis on environmental responsibility in the tech industry.</p></li>
</ol>
<p>By focusing on optimising RAG and embedding strategies for smaller LLMs, this research not only expands the accessibility and practical applications of language AI but also aligns with the broader goals of developing efficient, sustainable, and inclusive AI solutions. The insights and techniques derived from this work have the potential to inspire further advancements in the field of natural language processing.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="retrieval-augmented-generation-rag"><span class="header-section-number">3</span> Retrieval-Augmented Generation (RAG)</h2>
<p>RAG is a technique that improves the accuracy of generative models by enabling them to search through external data and fact-check before answering questions.</p>
<section id="rag-components" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="rag-components"><span class="header-section-number">3.1</span> RAG Components</h3>
<ol type="1">
<li><p><strong>Retriever</strong>: Searches the knowledge base for relevant information using text embeddings to respond accurately to user queries.</p></li>
<li><p><strong>Knowledge Base</strong>: A mutable, domain-specific knowledge base that can be updated with new information dynamically, overcoming the static knowledge limitation of traditionally trained LLMs.</p></li>
</ol>
</section>
<section id="rag-implementation" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="rag-implementation"><span class="header-section-number">3.2</span> RAG Implementation</h3>
<ol type="1">
<li><p><strong>Preprocessing</strong>: The RAG module preprocesses user queries by fetching relevant information from the knowledge base, enhancing the context before the query is processed by the LLM.</p></li>
<li><p><strong>Chunking and Embedding</strong>: The process involves loading documents, creating embeddings, and chunking content using advanced techniques like semantic or agentic chunking to ensure only the most relevant and contextually appropriate information is passed to the LLM.</p></li>
<li><p><strong>Tools and Libraries</strong>: Implementations often utilise Python libraries like Hugging Face for embeddings and indexing, as well as tools like LlamaIndex to automate chunking and summary attachment.</p></li>
</ol>
<p>By combining advanced text chunking strategies, text embeddings, and RAG systems, language models can provide more accurate and relevant responses, particularly in specialised or technical domains where static knowledge may be insufficient.</p>
</section>
</section>
<section id="text-embeddings" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="text-embeddings"><span class="header-section-number">4</span> Text Embeddings</h2>
<p>Text embeddings are numerical representations of text that capture semantic meanings, facilitating computational processing and analysis of textual data. They enable practical applications like:</p>
<ol type="1">
<li><p><strong>Text Classification</strong>: Training machine learning models on embedded text data to identify roles from job descriptions or detect fraudulent activities.</p></li>
<li><p><strong>Semantic Search</strong>: Understanding the contextual meaning behind queries, unlike keyword search, which merely looks for exact word matches.</p></li>
</ol>
<p>Text embeddings overcome the challenge of performing mathematical operations directly on text by mapping textual data into computable vectors or numbers. This allows for advanced techniques like visualising job description clusters, differentiating roles, and identifying similarities.</p>
<p>While large language models (LLMs) like ChatGPT can handle many text-based tasks, text embeddings are essential for specific, high-stakes applications due to lower computational costs and fewer security risks.</p>
</section>
<section id="methodology" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="methodology"><span class="header-section-number">5</span> Methodology</h2>
</section>
<section id="text-chunking-strategies" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="text-chunking-strategies"><span class="header-section-number">6</span> Text Chunking Strategies</h2>
<p>Text chunking involves dividing large datasets into smaller, manageable chunks, enabling language models to process information more effectively. By providing only the most contextually relevant information, chunking enhances response accuracy and relevance.</p>
<p>In this study, we explored and compared several text chunking strategies to enhance the performance and accuracy of language models, particularly in retrieval-augmented generation (RAG) applications. The following chunking techniques were employed:</p>
<section id="basic-chunking-methods" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="basic-chunking-methods"><span class="header-section-number">6.1</span> Basic Chunking Methods</h3>
<ol type="1">
<li><p><strong>Character Splitting</strong>: Dividing text strictly by character count, which can distort words and meanings, reducing response quality.</p></li>
<li><p><strong>Recursive Character Splitting</strong>: Using delimiters like new lines or specific characters to split text recursively, providing slightly more context than basic character splitting.</p></li>
<li><p><strong>Document-Based Chunking</strong>: Splitting text based on document types or structures, like Python code or Markdown, aiming to retain more context compared to basic methods.</p></li>
</ol>
</section>
<section id="advanced-chunking-techniques" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="advanced-chunking-techniques"><span class="header-section-number">6.2</span> Advanced Chunking Techniques</h3>
<ol start="4" type="1">
<li><p><strong>Semantic Chunking</strong>: Using embeddings to analyse the semantic relationship between text segments, grouping chunks based on meaning and significantly improving the relevancy of data chunks.</p></li>
<li><p><strong>Agentic Chunking</strong>: With this method chunks are processed using a large language model to ensure each chunk stands alone with complete meaning, enhancing coherence and context preservation.</p></li>
<li><p><strong>Subdocument Chunking</strong>: The subdocument RAG technique is highlighted as an advanced strategy. It summarises entire documents, attaches the summaries as metadata to each chunk’s embedding, and uses a hierarchical retrieval process searching summaries first to improve efficiency and accuracy.</p></li>
</ol>
</section>
<section id="semantic-chunking" class="level3" data-number="6.3">
<h3 data-number="6.3" class="anchored" data-anchor-id="semantic-chunking"><span class="header-section-number">6.3</span> Semantic Chunking</h3>
<p>Semantic chunking of text in large language models refers to the process of breaking down long pieces of text into smaller, meaningful chunks or segments based on the semantic content and context. This is an important step when working with large language models (LLMs) that have limited context windows and can only process a certain amount of text at a time.</p>
<p>The key aspects of semantic chunking are:</p>
<ol type="1">
<li><p>It aims to divide the text into coherent and contextually relevant segments, preserving the meaning and flow of ideas within each chunk.</p></li>
<li><p>The chunking is guided by the semantic understanding of the content, rather than relying solely on predefined rules like fixed chunk sizes or separators.</p></li>
<li><p>It leverages the language understanding capabilities of the LLM itself to identify optimal break points or boundaries for chunking, based on the model’s comprehension of the text’s structure, topics, and semantic coherence.</p></li>
<li><p>The goal is to produce chunks that are more meaningful and contextually relevant, compared to arbitrary chunking methods. This can lead to improved performance in downstream tasks like information retrieval, question answering, or text summarisation.</p></li>
</ol>
<p>The semantic chunking process typically involves:</p>
<ol type="1">
<li>Providing the full text to the LLM</li>
<li>Prompting the LLM to suggest chunk boundaries based on its understanding of the content</li>
<li>The LLM generates proposed chunk boundaries (e.g., character positions, sentence indices)</li>
<li>The text is split into chunks based on the LLM’s suggestions</li>
</ol>
<p>While semantic chunking can produce more coherent and relevant chunks, it also introduces additional complexity and computational overhead, as it requires running the LLM on the entire text corpus during the chunking process. The quality of the chunking results may depend on the specific prompting and fine-tuning of the LLM for this task.</p>
</section>
</section>
<section id="agentic-chunking" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="agentic-chunking"><span class="header-section-number">7</span> Agentic Chunking</h2>
<p>Agentic chunking is a text splitting strategy that explores the possibility of using a large language model (LLM) itself to determine how much and what text should be chunked together. It aims to leverage the LLM’s understanding of the content to make more informed decisions about where to split the text, rather than relying solely on predefined rules or heuristics.</p>
<p>In agentic chunking, the LLM is tasked with analysing the text and identifying optimal break points or boundaries for chunking. This process typically involves the following steps:</p>
<ol type="1">
<li>The LLM is provided with the full text or document.</li>
<li>The LLM is prompted to suggest chunk boundaries based on its understanding of the content’s structure, topics, and semantic coherence.</li>
<li>The LLM generates a set of proposed chunk boundaries, which can be in the form of character positions, sentence indices, or other markers.</li>
<li>The text is then split into chunks based on the LLM’s suggestions.</li>
</ol>
<p>The key advantage of agentic chunking is that it can potentially produce more semantically coherent and meaningful chunks compared to other methods like fixed-size or recursive chunking. By leveraging the LLM’s language understanding capabilities, agentic chunking can better preserve the context and meaning within each chunk, leading to improved performance in downstream tasks like information retrieval or question answering.</p>
<p>However, agentic chunking also introduces additional complexity and computational overhead, as it requires running the LLM on the entire text corpus during the chunking process. Additionally, the quality of the chunking results may depend on the specific prompting and fine-tuning of the LLM for this task.</p>
<section id="subdocument-rag-technique" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="subdocument-rag-technique"><span class="header-section-number">7.1</span> Subdocument RAG Technique</h3>
<p>The subdocument RAG technique aims to improve the accuracy and efficiency of retrieval-augmented generation (RAG) systems by using document summaries to enhance the retrieval process.</p>
<p>Traditional chunking methods lack global context awareness, as each chunk only contains a part of a document’s context. The subdocument strategy addresses this by summarising entire documents or large segments, and attaching these summaries as metadata to each chunk’s embedding in the vector database.</p>
<p>When a user query is received, the system first searches through the document summaries to find relevant documents related to the query. It then retrieves specific chunks from these relevant documents, enhancing both efficiency by reducing the number of chunks searched, and accuracy by providing more contextual information.</p>
<p>This hierarchical retrieval strategy implements a two-step process: 1) Summarising and dividing documents into subdocuments, and 2) Attaching these summaries to related chunks in the database. For example, in a book on machine learning, chapters could be treated as subdocuments, with each chapter’s summary aiding in retrieving relevant chunks within that chapter.</p>
<p>The subdocument technique is superior to basic RAG setups without summaries, as it improves response times, relevance, and overcomes the lack of global context in traditional chunking methods. Tools like LlamaIndex simplify implementation by automating the chunking and summary attachment process.</p>
<p>Yes, it would be appropriate to include a section or subsection describing the evaluation methods used to assess the performance and effectiveness of the different text chunking strategies discussed in the white paper.</p>
<p>Here’s how you could structure this section:</p>
</section>
</section>
<section id="evaluation-methods" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="evaluation-methods"><span class="header-section-number">8</span> Evaluation Methods</h2>
<p>To comprehensively evaluate the various text chunking strategies and their impact on language model performance, we employed the following evaluation methods:</p>
<section id="chunk-coherence-and-relevance" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="chunk-coherence-and-relevance"><span class="header-section-number">8.1</span> Chunk Coherence and Relevance</h3>
<ol type="1">
<li><p><strong>Coherence Scores</strong>: We utilised topic modelling algorithms to compute coherence scores for the chunks produced by each chunking strategy. Higher coherence scores indicate that the chunks are more semantically coherent and contextually relevant.</p></li>
<li><p><strong>Human Evaluation</strong>: A subset of the generated chunks was manually evaluated by human annotators, who assigned scores based on the quality, meaning preservation, and contextual relevance of the chunks.</p></li>
</ol>
</section>
<section id="downstream-task-performance" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="downstream-task-performance"><span class="header-section-number">8.2</span> Downstream Task Performance</h3>
<p>The effectiveness of the chunking strategies was further assessed by evaluating the performance of language models on downstream tasks when using the generated chunks:</p>
<ol type="1">
<li><p><strong>Question Answering</strong>: We measured the accuracy and F1 scores of language models on question-answering tasks, using chunks from different chunking methods as input.</p></li>
<li><p><strong>Information Retrieval</strong>: The chunks were used to build retrieval systems, and we evaluated the precision, recall, and mean reciprocal rank (MRR) of these systems on relevant information retrieval benchmarks.</p></li>
<li><p><strong>Text Summarisation</strong>: We assessed the quality of summaries generated by language models when provided with chunks from various chunking strategies, using metrics like ROUGE scores and human evaluation.</p></li>
</ol>
</section>
<section id="efficiency-metrics" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="efficiency-metrics"><span class="header-section-number">8.3</span> Efficiency Metrics</h3>
<p>To understand the computational efficiency and scalability of the chunking strategies, we measured and compared the following metrics:</p>
<ol type="1">
<li><p><strong>Chunking Time</strong>: The time taken by each chunking strategy to process a given corpus or dataset, plotted against the dataset size to assess scalability.</p></li>
<li><p><strong>Memory Usage</strong>: The memory footprint of each chunking strategy during the chunking process, which is crucial for resource-constrained environments or large-scale applications.</p></li>
</ol>
<p>By combining these evaluation methods, we aimed to provide a comprehensive analysis of the strengths and weaknesses of each text chunking strategy, considering both the quality of the generated chunks and their impact on downstream language model performance, as well as the computational efficiency and scalability of the approaches.</p>
</section>
</section>
<section id="future-work" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="future-work"><span class="header-section-number">9</span> Future Work</h2>
<p>While the techniques discussed in this paper have shown promising results in enhancing the performance and accuracy of language models, there are several areas that warrant further exploration and research:</p>
<ol type="1">
<li><p><strong>Multimodal Embeddings</strong>: Extending text embeddings to incorporate multimodal data, such as images, videos, and audio, could unlock new applications and improve the contextual understanding of language models.</p></li>
<li><p><strong>Unsupervised Chunking</strong>: Developing unsupervised methods for text chunking that can automatically identify optimal chunk boundaries without relying on labeled data or human intervention could lead to more scalable and efficient solutions.</p></li>
<li><p><strong>Adaptive Chunking</strong>: Exploring adaptive chunking strategies that can dynamically adjust the chunk size and granularity based on the complexity of the content or the specific task at hand could further improve the trade-off between computational efficiency and contextual relevance.</p></li>
<li><p><strong>Incremental Knowledge Base Updates</strong>: Investigating techniques for seamlessly integrating new information into existing knowledge bases without the need for complete retraining or rebuilding could enhance the adaptability and scalability of retrieval-augmented generation systems.</p></li>
<li><p><strong>Explainable Chunking and Retrieval</strong>: Developing methods to provide interpretable explanations for the chunking and retrieval decisions made by language models could increase transparency and trust in these systems, particularly in high-stakes applications.</p></li>
<li><p><strong>Integration with Knowledge Graphs</strong>: Investigating the integration of structured knowledge graphs with retrieval-augmented generation systems could unlock new possibilities for leveraging structured data and enhancing the contextual understanding of language models. Techniques for seamlessly incorporating knowledge graph information into the retrieval and generation processes could be explored.</p></li>
</ol>
</section>
<section id="conclusion" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">10</span> Conclusion</h2>
<p>The integration of advanced text chunking strategies and text embeddings has proven to be a powerful approach for enhancing the performance and accuracy of language models, particularly in retrieval-augmented generation applications. By effectively managing and processing large volumes of textual data, these techniques enable language models to provide more relevant and contextually appropriate responses.</p>
<p>The subdocument RAG technique, which leverages document summaries to improve retrieval efficiency and contextual relevance, has emerged as a promising solution for overcoming the limitations of traditional chunking methods. By combining advanced chunking techniques with text embeddings and the RAG framework, language models can better leverage dynamic, domain-specific knowledge bases, leading to more accurate and reliable responses.</p>
<p>As natural language processing continues to evolve, further research and development in areas such as multimodal embeddings, unsupervised chunking, adaptive chunking, incremental knowledge base updates, and explainable chunking and retrieval will be crucial for unlocking the full potential of these techniques and addressing the ever-increasing complexity of textual data.</p>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{borck2024,
  author = {Borck, Michael},
  title = {Optimising {Language} {Models} with {Advanced} {Text}
    {Chunking} {Strategies}},
  date = {2024-04-24},
  langid = {en},
  abstract = {This white paper explores advanced text chunking
    strategies and text embeddings to optimise the performance and
    accuracy of language models, particularly for retrieval-augmented
    generation (RAG) applications. It examines various chunking
    techniques from basic character splitting to advanced semantic and
    agentic methods that leverage language models to identify meaningful
    chunk boundaries based on content understanding. The paper provides
    an in-depth analysis of the RAG framework, enabling language models
    to search external, dynamic knowledge bases and incorporate relevant
    information into responses. Emphasis is placed on the subdocument
    RAG technique, which summarises entire documents, attaches summaries
    as metadata to chunk embeddings, and employs hierarchical retrieval
    searching summaries first for improved efficiency and contextual
    relevance. By combining these techniques, the paper demonstrates how
    language models can leverage dynamic knowledge bases to provide
    accurate, contextually relevant responses, paving the way for
    further advancements like multimodal embeddings, unsupervised
    chunking, adaptive chunking, incremental knowledge updates,
    explainable retrieval, and knowledge graph integration.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-borck2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Borck, Michael. 2024. <span>“Optimising Language Models with Advanced
Text Chunking Strategies.”</span> BARG Curtin University. April 24,
2024.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>