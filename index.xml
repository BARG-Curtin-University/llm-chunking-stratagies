<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
  <front>
    <journal-meta>
      <journal-id/>
      <journal-title-group>
        <journal-title>BARG Curtin University</journal-title>
      </journal-title-group>
      <issn/>
      <publisher>
        <publisher-name/>
      </publisher>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Optimising Language Models with Advanced Text Chunking
Strategies</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <contrib-id contrib-id-type="orcid">0000-0002-0950-6396</contrib-id>
          <name>
            <surname>Borck</surname>
            <given-names>Michael</given-names>
          </name>
          <string-name>Michael Borck</string-name>
          <email>michael.borck@curtin.edu.au</email>
          <role vocab="https://credit.niso.org" vocab-term="investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="https://credit.niso.org" vocab-term="project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project
administration</role>
          <role vocab="https://credit.niso.org" vocab-term="software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role>
          <role>Visualisation</role>
          <xref ref-type="aff" rid="aff-1">a</xref>
          <xref ref-type="corresp" rid="cor-1">*</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1">
        <institution-wrap>
          <institution>Curtin University</institution>
        </institution-wrap>
      </aff>
      <author-notes>
        <corresp id="cor-1">michael.borck@curtin.edu.au</corresp>
      </author-notes>
      <pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-04-24">
        <year>2024</year>
        <month>4</month>
        <day>24</day>
      </pub-date>
      <history/>
      <abstract>
        <p>This white paper explores advanced text chunking strategies and text
embeddings to optimise the performance and accuracy of language models,
particularly for retrieval-augmented generation (RAG) applications. It
examines various chunking techniques from basic character splitting to
advanced semantic and agentic methods that leverage language models to
identify meaningful chunk boundaries based on content understanding. The
paper provides an in-depth analysis of the RAG framework, enabling
language models to search external, dynamic knowledge bases and
incorporate relevant information into responses. Emphasis is placed on
the subdocument RAG technique, which summarises entire documents,
attaches summaries as metadata to chunk embeddings, and employs
hierarchical retrieval searching summaries first for improved efficiency
and contextual relevance. By combining these techniques, the paper
demonstrates how language models can leverage dynamic knowledge bases to
provide accurate, contextually relevant responses, paving the way for
further advancements like multimodal embeddings, unsupervised chunking,
adaptive chunking, incremental knowledge updates, explainable retrieval,
and knowledge graph integration.</p>
      </abstract>
      <kwd-group kwd-group-type="author">
        <kwd>Text Chunking Strategies</kwd>
        <kwd>Text Embeddings 1</kwd>
        <kwd>Retrieval-Augmented Generation (RAG)</kwd>
        <kwd>Language Model Optimisation</kwd>
        <kwd>Semantic Chunking</kwd>
        <kwd>Subdocument Chunking</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="introduction">
      <title>1 Introduction</title>
      <p>As natural language processing (NLP) continues to evolve,
  optimising language model performance has become paramount. However,
  the sheer volume and complexity of textual data often hinder model
  performance. Text chunking strategies and text embeddings have emerged
  as crucial components to address these challenges.</p>
    </sec>
    <sec id="contributions-and-significance">
      <title>2 Contributions and Significance</title>
      <p>This research makes several notable contributions to the field of
  natural language processing and language model optimisation:</p>
      <list list-type="order">
        <list-item>
          <p><bold>Democratising Language AI with Small LLMs</bold>: By
      focusing on optimising retrieval-augmented generation (RAG) and
      text embedding strategies for smaller, quantised open-source
      language models (around 4GB in size), this work aims to
      democratise language AI and make it more accessible to a broader
      range of users with limited computational resources or budgets.
      This approach aligns with the growing trend of developing
      efficient and environmentally-friendly AI solutions, reducing the
      carbon footprint and energy consumption associated with
      large-scale language models.</p>
        </list-item>
        <list-item>
          <p><bold>Practical Applications and Accessibility</bold>: Many
      real-world applications, such as chatbots, virtual assistants, and
      domain-specific language models, may not require the full
      capabilities of large LLMs. By optimising smaller models, this
      research enables practical and cost-effective solutions for a wide
      range of use cases, fostering innovation and inclusivity in
      language AI.</p>
        </list-item>
        <list-item>
          <p><bold>Exploration of Novel Techniques</bold>: The constraints
      imposed by working with smaller LLMs can drive innovation and
      potentially uncover novel techniques or insights that could be
      applied to larger models or inspire new research directions in the
      field of language AI.</p>
        </list-item>
        <list-item>
          <p><bold>Reproducibility and Collaboration</bold>: The use of
      open-source models and quantisation techniques promotes
      reproducibility and collaboration within the research community,
      enabling others to build upon this work and contribute to the
      collective advancement of language AI.</p>
        </list-item>
        <list-item>
          <p><bold>Efficient and Sustainable AI</bold>: Smaller LLMs have
      lower computational requirements, reducing the energy consumption
      and carbon footprint associated with training and deploying
      language models. This research contributes to the development of
      more efficient and sustainable AI solutions, aligning with the
      growing emphasis on environmental responsibility in the tech
      industry.</p>
        </list-item>
      </list>
      <p>By focusing on optimising RAG and embedding strategies for smaller
  LLMs, this research not only expands the accessibility and practical
  applications of language AI but also aligns with the broader goals of
  developing efficient, sustainable, and inclusive AI solutions. The
  insights and techniques derived from this work have the potential to
  inspire further advancements in the field of natural language
  processing.</p>
    </sec>
    <sec id="retrieval-augmented-generation-rag">
      <title>3 Retrieval-Augmented Generation (RAG)</title>
      <p>RAG is a technique that improves the accuracy of generative models
  by enabling them to search through external data and fact-check before
  answering questions.</p>
      <sec id="rag-components">
        <title>3.1 RAG Components</title>
        <list list-type="order">
          <list-item>
            <p><bold>Retriever</bold>: Searches the knowledge base for
        relevant information using text embeddings to respond accurately
        to user queries.</p>
          </list-item>
          <list-item>
            <p><bold>Knowledge Base</bold>: A mutable, domain-specific
        knowledge base that can be updated with new information
        dynamically, overcoming the static knowledge limitation of
        traditionally trained LLMs.</p>
          </list-item>
        </list>
      </sec>
      <sec id="rag-implementation">
        <title>3.2 RAG Implementation</title>
        <list list-type="order">
          <list-item>
            <p><bold>Preprocessing</bold>: The RAG module preprocesses user
        queries by fetching relevant information from the knowledge
        base, enhancing the context before the query is processed by the
        LLM.</p>
          </list-item>
          <list-item>
            <p><bold>Chunking and Embedding</bold>: The process involves
        loading documents, creating embeddings, and chunking content
        using advanced techniques like semantic or agentic chunking to
        ensure only the most relevant and contextually appropriate
        information is passed to the LLM.</p>
          </list-item>
          <list-item>
            <p><bold>Tools and Libraries</bold>: Implementations often
        utilise Python libraries like Hugging Face for embeddings and
        indexing, as well as tools like LlamaIndex to automate chunking
        and summary attachment.</p>
          </list-item>
        </list>
        <p>By combining advanced text chunking strategies, text embeddings,
    and RAG systems, language models can provide more accurate and
    relevant responses, particularly in specialised or technical domains
    where static knowledge may be insufficient.</p>
      </sec>
    </sec>
    <sec id="text-embeddings">
      <title>4 Text Embeddings</title>
      <p>Text embeddings are numerical representations of text that capture
  semantic meanings, facilitating computational processing and analysis
  of textual data. They enable practical applications like:</p>
      <list list-type="order">
        <list-item>
          <p><bold>Text Classification</bold>: Training machine learning
      models on embedded text data to identify roles from job
      descriptions or detect fraudulent activities.</p>
        </list-item>
        <list-item>
          <p><bold>Semantic Search</bold>: Understanding the contextual
      meaning behind queries, unlike keyword search, which merely looks
      for exact word matches.</p>
        </list-item>
      </list>
      <p>Text embeddings overcome the challenge of performing mathematical
  operations directly on text by mapping textual data into computable
  vectors or numbers. This allows for advanced techniques like
  visualising job description clusters, differentiating roles, and
  identifying similarities.</p>
      <p>While large language models (LLMs) like ChatGPT can handle many
  text-based tasks, text embeddings are essential for specific,
  high-stakes applications due to lower computational costs and fewer
  security risks.</p>
    </sec>
    <sec id="methodology">
      <title>5 Methodology</title>
    </sec>
    <sec id="text-chunking-strategies">
      <title>6 Text Chunking Strategies</title>
      <p>Text chunking involves dividing large datasets into smaller,
  manageable chunks, enabling language models to process information
  more effectively. By providing only the most contextually relevant
  information, chunking enhances response accuracy and relevance.</p>
      <p>In this study, we explored and compared several text chunking
  strategies to enhance the performance and accuracy of language models,
  particularly in retrieval-augmented generation (RAG) applications. The
  following chunking techniques were employed:</p>
      <sec id="basic-chunking-methods">
        <title>6.1 Basic Chunking Methods</title>
        <list list-type="order">
          <list-item>
            <p><bold>Character Splitting</bold>: Dividing text strictly by
        character count, which can distort words and meanings, reducing
        response quality.</p>
          </list-item>
          <list-item>
            <p><bold>Recursive Character Splitting</bold>: Using delimiters
        like new lines or specific characters to split text recursively,
        providing slightly more context than basic character
        splitting.</p>
          </list-item>
          <list-item>
            <p><bold>Document-Based Chunking</bold>: Splitting text based on
        document types or structures, like Python code or Markdown,
        aiming to retain more context compared to basic methods.</p>
          </list-item>
        </list>
      </sec>
      <sec id="advanced-chunking-techniques">
        <title>6.2 Advanced Chunking Techniques</title>
        <list list-type="order">
          <list-item>
            <label>4.</label>
            <p><bold>Semantic Chunking</bold>: Using embeddings to analyse
        the semantic relationship between text segments, grouping chunks
        based on meaning and significantly improving the relevancy of
        data chunks.</p>
          </list-item>
          <list-item>
            <label>5.</label>
            <p><bold>Agentic Chunking</bold>: With this method chunks are
        processed using a large language model to ensure each chunk
        stands alone with complete meaning, enhancing coherence and
        context preservation.</p>
          </list-item>
          <list-item>
            <label>6.</label>
            <p><bold>Subdocument Chunking</bold>: The subdocument RAG
        technique is highlighted as an advanced strategy. It summarises
        entire documents, attaches the summaries as metadata to each
        chunk’s embedding, and uses a hierarchical retrieval process
        searching summaries first to improve efficiency and
        accuracy.</p>
          </list-item>
        </list>
      </sec>
      <sec id="semantic-chunking">
        <title>6.3 Semantic Chunking</title>
        <p>Semantic chunking of text in large language models refers to the
    process of breaking down long pieces of text into smaller,
    meaningful chunks or segments based on the semantic content and
    context. This is an important step when working with large language
    models (LLMs) that have limited context windows and can only process
    a certain amount of text at a time.</p>
        <p>The key aspects of semantic chunking are:</p>
        <list list-type="order">
          <list-item>
            <p>It aims to divide the text into coherent and contextually
        relevant segments, preserving the meaning and flow of ideas
        within each chunk.</p>
          </list-item>
          <list-item>
            <p>The chunking is guided by the semantic understanding of the
        content, rather than relying solely on predefined rules like
        fixed chunk sizes or separators.</p>
          </list-item>
          <list-item>
            <p>It leverages the language understanding capabilities of the
        LLM itself to identify optimal break points or boundaries for
        chunking, based on the model’s comprehension of the text’s
        structure, topics, and semantic coherence.</p>
          </list-item>
          <list-item>
            <p>The goal is to produce chunks that are more meaningful and
        contextually relevant, compared to arbitrary chunking methods.
        This can lead to improved performance in downstream tasks like
        information retrieval, question answering, or text
        summarisation.</p>
          </list-item>
        </list>
        <p>The semantic chunking process typically involves:</p>
        <list list-type="order">
          <list-item>
            <p>Providing the full text to the LLM</p>
          </list-item>
          <list-item>
            <p>Prompting the LLM to suggest chunk boundaries based on its
        understanding of the content</p>
          </list-item>
          <list-item>
            <p>The LLM generates proposed chunk boundaries (e.g., character
        positions, sentence indices)</p>
          </list-item>
          <list-item>
            <p>The text is split into chunks based on the LLM’s
        suggestions</p>
          </list-item>
        </list>
        <p>While semantic chunking can produce more coherent and relevant
    chunks, it also introduces additional complexity and computational
    overhead, as it requires running the LLM on the entire text corpus
    during the chunking process. The quality of the chunking results may
    depend on the specific prompting and fine-tuning of the LLM for this
    task.</p>
      </sec>
    </sec>
    <sec id="agentic-chunking">
      <title>7 Agentic Chunking</title>
      <p>Agentic chunking is a text splitting strategy that explores the
  possibility of using a large language model (LLM) itself to determine
  how much and what text should be chunked together. It aims to leverage
  the LLM’s understanding of the content to make more informed decisions
  about where to split the text, rather than relying solely on
  predefined rules or heuristics.</p>
      <p>In agentic chunking, the LLM is tasked with analysing the text and
  identifying optimal break points or boundaries for chunking. This
  process typically involves the following steps:</p>
      <list list-type="order">
        <list-item>
          <p>The LLM is provided with the full text or document.</p>
        </list-item>
        <list-item>
          <p>The LLM is prompted to suggest chunk boundaries based on its
      understanding of the content’s structure, topics, and semantic
      coherence.</p>
        </list-item>
        <list-item>
          <p>The LLM generates a set of proposed chunk boundaries, which can
      be in the form of character positions, sentence indices, or other
      markers.</p>
        </list-item>
        <list-item>
          <p>The text is then split into chunks based on the LLM’s
      suggestions.</p>
        </list-item>
      </list>
      <p>The key advantage of agentic chunking is that it can potentially
  produce more semantically coherent and meaningful chunks compared to
  other methods like fixed-size or recursive chunking. By leveraging the
  LLM’s language understanding capabilities, agentic chunking can better
  preserve the context and meaning within each chunk, leading to
  improved performance in downstream tasks like information retrieval or
  question answering.</p>
      <p>However, agentic chunking also introduces additional complexity and
  computational overhead, as it requires running the LLM on the entire
  text corpus during the chunking process. Additionally, the quality of
  the chunking results may depend on the specific prompting and
  fine-tuning of the LLM for this task.</p>
      <sec id="subdocument-rag-technique">
        <title>7.1 Subdocument RAG Technique</title>
        <p>The subdocument RAG technique aims to improve the accuracy and
    efficiency of retrieval-augmented generation (RAG) systems by using
    document summaries to enhance the retrieval process.</p>
        <p>Traditional chunking methods lack global context awareness, as
    each chunk only contains a part of a document’s context. The
    subdocument strategy addresses this by summarising entire documents
    or large segments, and attaching these summaries as metadata to each
    chunk’s embedding in the vector database.</p>
        <p>When a user query is received, the system first searches through
    the document summaries to find relevant documents related to the
    query. It then retrieves specific chunks from these relevant
    documents, enhancing both efficiency by reducing the number of
    chunks searched, and accuracy by providing more contextual
    information.</p>
        <p>This hierarchical retrieval strategy implements a two-step
    process: 1) Summarising and dividing documents into subdocuments,
    and 2) Attaching these summaries to related chunks in the database.
    For example, in a book on machine learning, chapters could be
    treated as subdocuments, with each chapter’s summary aiding in
    retrieving relevant chunks within that chapter.</p>
        <p>The subdocument technique is superior to basic RAG setups without
    summaries, as it improves response times, relevance, and overcomes
    the lack of global context in traditional chunking methods. Tools
    like LlamaIndex simplify implementation by automating the chunking
    and summary attachment process.</p>
        <p>Yes, it would be appropriate to include a section or subsection
    describing the evaluation methods used to assess the performance and
    effectiveness of the different text chunking strategies discussed in
    the white paper.</p>
        <p>Here’s how you could structure this section:</p>
      </sec>
    </sec>
    <sec id="evaluation-methods">
      <title>8 Evaluation Methods</title>
      <p>To comprehensively evaluate the various text chunking strategies
  and their impact on language model performance, we employed the
  following evaluation methods:</p>
      <sec id="chunk-coherence-and-relevance">
        <title>8.1 Chunk Coherence and Relevance</title>
        <list list-type="order">
          <list-item>
            <p><bold>Coherence Scores</bold>: We utilised topic modelling
        algorithms to compute coherence scores for the chunks produced
        by each chunking strategy. Higher coherence scores indicate that
        the chunks are more semantically coherent and contextually
        relevant.</p>
          </list-item>
          <list-item>
            <p><bold>Human Evaluation</bold>: A subset of the generated
        chunks was manually evaluated by human annotators, who assigned
        scores based on the quality, meaning preservation, and
        contextual relevance of the chunks.</p>
          </list-item>
        </list>
      </sec>
      <sec id="downstream-task-performance">
        <title>8.2 Downstream Task Performance</title>
        <p>The effectiveness of the chunking strategies was further assessed
    by evaluating the performance of language models on downstream tasks
    when using the generated chunks:</p>
        <list list-type="order">
          <list-item>
            <p><bold>Question Answering</bold>: We measured the accuracy and
        F1 scores of language models on question-answering tasks, using
        chunks from different chunking methods as input.</p>
          </list-item>
          <list-item>
            <p><bold>Information Retrieval</bold>: The chunks were used to
        build retrieval systems, and we evaluated the precision, recall,
        and mean reciprocal rank (MRR) of these systems on relevant
        information retrieval benchmarks.</p>
          </list-item>
          <list-item>
            <p><bold>Text Summarisation</bold>: We assessed the quality of
        summaries generated by language models when provided with chunks
        from various chunking strategies, using metrics like ROUGE
        scores and human evaluation.</p>
          </list-item>
        </list>
      </sec>
      <sec id="efficiency-metrics">
        <title>8.3 Efficiency Metrics</title>
        <p>To understand the computational efficiency and scalability of the
    chunking strategies, we measured and compared the following
    metrics:</p>
        <list list-type="order">
          <list-item>
            <p><bold>Chunking Time</bold>: The time taken by each chunking
        strategy to process a given corpus or dataset, plotted against
        the dataset size to assess scalability.</p>
          </list-item>
          <list-item>
            <p><bold>Memory Usage</bold>: The memory footprint of each
        chunking strategy during the chunking process, which is crucial
        for resource-constrained environments or large-scale
        applications.</p>
          </list-item>
        </list>
        <p>By combining these evaluation methods, we aimed to provide a
    comprehensive analysis of the strengths and weaknesses of each text
    chunking strategy, considering both the quality of the generated
    chunks and their impact on downstream language model performance, as
    well as the computational efficiency and scalability of the
    approaches.</p>
      </sec>
    </sec>
    <sec id="future-work">
      <title>9 Future Work</title>
      <p>While the techniques discussed in this paper have shown promising
  results in enhancing the performance and accuracy of language models,
  there are several areas that warrant further exploration and
  research:</p>
      <list list-type="order">
        <list-item>
          <p><bold>Multimodal Embeddings</bold>: Extending text embeddings
      to incorporate multimodal data, such as images, videos, and audio,
      could unlock new applications and improve the contextual
      understanding of language models.</p>
        </list-item>
        <list-item>
          <p><bold>Unsupervised Chunking</bold>: Developing unsupervised
      methods for text chunking that can automatically identify optimal
      chunk boundaries without relying on labeled data or human
      intervention could lead to more scalable and efficient
      solutions.</p>
        </list-item>
        <list-item>
          <p><bold>Adaptive Chunking</bold>: Exploring adaptive chunking
      strategies that can dynamically adjust the chunk size and
      granularity based on the complexity of the content or the specific
      task at hand could further improve the trade-off between
      computational efficiency and contextual relevance.</p>
        </list-item>
        <list-item>
          <p><bold>Incremental Knowledge Base Updates</bold>: Investigating
      techniques for seamlessly integrating new information into
      existing knowledge bases without the need for complete retraining
      or rebuilding could enhance the adaptability and scalability of
      retrieval-augmented generation systems.</p>
        </list-item>
        <list-item>
          <p><bold>Explainable Chunking and Retrieval</bold>: Developing
      methods to provide interpretable explanations for the chunking and
      retrieval decisions made by language models could increase
      transparency and trust in these systems, particularly in
      high-stakes applications.</p>
        </list-item>
        <list-item>
          <p><bold>Integration with Knowledge Graphs</bold>: Investigating
      the integration of structured knowledge graphs with
      retrieval-augmented generation systems could unlock new
      possibilities for leveraging structured data and enhancing the
      contextual understanding of language models. Techniques for
      seamlessly incorporating knowledge graph information into the
      retrieval and generation processes could be explored.</p>
        </list-item>
      </list>
    </sec>
    <sec id="conclusion">
      <title>10 Conclusion</title>
      <p>The integration of advanced text chunking strategies and text
  embeddings has proven to be a powerful approach for enhancing the
  performance and accuracy of language models, particularly in
  retrieval-augmented generation applications. By effectively managing
  and processing large volumes of textual data, these techniques enable
  language models to provide more relevant and contextually appropriate
  responses.</p>
      <p>The subdocument RAG technique, which leverages document summaries
  to improve retrieval efficiency and contextual relevance, has emerged
  as a promising solution for overcoming the limitations of traditional
  chunking methods. By combining advanced chunking techniques with text
  embeddings and the RAG framework, language models can better leverage
  dynamic, domain-specific knowledge bases, leading to more accurate and
  reliable responses.</p>
      <p>As natural language processing continues to evolve, further
  research and development in areas such as multimodal embeddings,
  unsupervised chunking, adaptive chunking, incremental knowledge base
  updates, and explainable chunking and retrieval will be crucial for
  unlocking the full potential of these techniques and addressing the
  ever-increasing complexity of textual data.</p>
    </sec>
  </body>
  <back>
</back>
  <sub-article article-type="notebook" id="nb-3-nb-1">
    <front-stub>
      <title-group>
        <article-title>Sub-Document Summary Metadata Pack</article-title>
      </title-group>
    </front-stub>
    <body>
      <sec id="cell-8dd0acdb-5aec-4129-8772-81f56d6b25cf-nb-1" specific-use="notebook-content">
        <p>This LlamaPack provides an advanced technique for injecting each
chunk with “sub-document” metadata. This context augmentation technique
is helpful for both retrieving relevant context and for synthesizing
correct answers.</p>
        <p>It is a step beyond simply adding a summary of the document as the
metadata to each chunk. Within a long document, there can be multiple
distinct themes, and we want each chunk to be grounded in global but
relevant context.</p>
        <p>Source:
https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb
Video: https://www.youtube.com/watch?v=m6P1Rp91AzM&amp;t=1s</p>
      </sec>
      <sec id="cell-66818da6-a3fb-4537-b30a-922a8a0ef99e-nb-1" specific-use="notebook-content">
        <sec id="setup-data-nb-1">
          <title>Setup Data</title>
        </sec>
        <sec id="cell-317a3207-1211-4a6a-bd7d-3ab14f399951-nb-1" specific-use="notebook-code">
          <code language="python">!mkdir -p 'data/'
!curl 'https://arxiv.org/pdf/2307.09288.pdf' -o 'data/llama2.pdf'</code>
          <sec id="cell-317a3207-1211-4a6a-bd7d-3ab14f399951-output-0-nb-1" specific-use="notebook-output">
            <preformat>811.82s - pydevd: Sending message related to process being replaced timed-out after 5 seconds
817.00s - pydevd: Sending message related to process being replaced timed-out after 5 seconds</preformat>
          </sec>
          <sec id="cell-317a3207-1211-4a6a-bd7d-3ab14f399951-output-1-nb-1" specific-use="notebook-output">
            <preformat>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 13.0M  100 13.0M    0     0  27.7M      0 --:--:-- --:--:-- --:--:-- 28.0M</preformat>
          </sec>
        </sec>
        <sec id="bf6ab9c0-c993-4ab2-8343-b294676d7550-nb-1" specific-use="notebook-code">
          <code language="python">from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader("data").load_data()</code>
        </sec>
        <sec id="cell-98bfbe4b-539c-469c-82e6-1f823f28d5f4-nb-1" specific-use="notebook-content">
</sec>
        <sec id="run-the-sub-document-summary-metadata-pack-nb-1">
          <title>Run the Sub-Document Summary Metadata Pack</title>
        </sec>
        <sec id="af4b815e-f5ce-406b-9dcb-5a23fc9f96db-nb-1" specific-use="notebook-code">
          <code language="python">%pip install llama-index-packs-subdoc-summary llama-index-llms-openai llama-index-embeddings-openai</code>
        </sec>
        <sec id="d619362b-ae45-4e47-b400-1c2ce7262496-nb-1" specific-use="notebook-code">
          <code language="python">from llama_index.packs.subdoc_summary import SubDocSummaryPack
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

subdoc_summary_pack = SubDocSummaryPack(
    documents,
    parent_chunk_size=8192,  # default,
    child_chunk_size=512,  # default
    llm=OpenAI(model="gpt-3.5-turbo"),
    embed_model=OpenAIEmbedding(),
)</code>
        </sec>
        <sec id="fb11a60d-d356-40c5-84c1-4135382bfbfd-nb-1" specific-use="notebook-code">
          <code language="python">from IPython.display import Markdown, display
from llama_index.core.response.notebook_utils import display_source_node

response = subdoc_summary_pack.run("How was Llama2 pretrained?")
display(Markdown(str(response)))
for n in response.source_nodes:
    display_source_node(n, source_length=10000, metadata_mode="all")</code>
          <sec id="fb11a60d-d356-40c5-84c1-4135382bfbfd-output-0-nb-1" specific-use="notebook-output">
            <p>Llama 2 was pretrained using an optimized auto-regressive
  transformer with robust data cleaning, updated data mixes, training on
  40% more total tokens, doubling the context length, and using
  grouped-query attention to improve inference scalability for larger
  models.</p>
          </sec>
          <sec id="fb11a60d-d356-40c5-84c1-4135382bfbfd-output-1-nb-1" specific-use="notebook-output">
            <p><bold>Node ID:</bold>
  172a1344-d48d-443b-8383-677037570c06<bold>Similarity:</bold>
  0.8720929924174893<bold>Text:</bold> page_label: 1 file_name:
  llama2.pdf file_path: data/llama2.pdf file_type: application/pdf
  file_size: 13661300 creation_date: 2024-02-17 last_modified_date:
  2024-02-17 last_accessed_date: 2024-02-17 context_summary: Llama 2 is
  a collection of pretrained and fine-tuned large language models
  optimized for dialogue use cases, ranging from 7 billion to 70 billion
  parameters. The models, known as Llama 2-Chat, have shown superior
  performance compared to open-source chat models on various benchmarks
  and are considered as potential alternatives to closed-source
  models.</p>
            <p>Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo
  Touvron∗Louis Martin†Kevin Stone† Peter Albert Amjad Almahairi Yasmine
  Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale
  Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem
  Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
  Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar
  Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa
  Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux
  Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier
  Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew
  Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten
  Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan
  Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng
  Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan
  Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗
  GenAI, Meta Abstract In this work, we develop and release Llama 2, a
  collection of pretrained and fine-tuned large language models (LLMs)
  ranging in scale from 7 billion to 70 billion parameters. Our
  fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use
  cases. Our models outperform open-source chat models on most
  benchmarks we tested, and based on
  ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
  source models. We provide a detailed description of our approach to
  fine-tuning and safety improvements of Llama 2-Chat in order to enable
  the community to build on our work and contribute to the responsible
  development of LLMs.</p>
          </sec>
          <sec id="fb11a60d-d356-40c5-84c1-4135382bfbfd-output-2-nb-1" specific-use="notebook-output">
            <p><bold>Node ID:</bold>
  dbbde2a7-d51c-4245-959d-ba97ba414b55<bold>Similarity:</bold>
  0.8700958215249326<bold>Text:</bold> page_label: 5 file_name:
  llama2.pdf file_path: data/llama2.pdf file_type: application/pdf
  file_size: 13661300 creation_date: 2024-02-17 last_modified_date:
  2024-02-17 last_accessed_date: 2024-02-17 context_summary: Llama
  2-Chat is developed through pretraining, supervised fine-tuning, and
  reinforcement learning with human feedback methodologies, focusing on
  refining the model iteratively. The training process involves using an
  optimized auto-regressive transformer, robust data cleaning, updated
  data mixes, and specific architectural enhancements like increased
  context length and grouped-query attention.</p>
            <p>Figure4: Trainingof Llama 2-Chat : Thisprocessbeginswiththe
  pretraining ofLlama 2 usingpublicly availableonlinesources.
  Followingthis,wecreateaninitialversionof Llama 2-Chat
  throughtheapplication ofsupervised fine-tuning . Subsequently, the
  model is iteratively refined using Reinforcement Learning with Human
  Feedback (RLHF) methodologies, specifically through rejection sampling
  and Proximal Policy
  Optimization(PPO).ThroughouttheRLHFstage,theaccumulationof
  iterativerewardmodelingdata in parallel with model enhancements is
  crucial to ensure the reward models remain within distribution. 2
  Pretraining Tocreatethenewfamilyof Llama
  2models,webeganwiththepretrainingapproachdescribedinTouvronetal.
  (2023), using an optimized auto-regressive transformer, but made
  several changes to improve performance.
  Specifically,weperformedmorerobustdatacleaning,updatedourdatamixes,trainedon40%moretotal
  tokens,doubledthecontextlength,andusedgrouped-queryattention(GQA)toimproveinferencescalability
  for our larger models. Table 1 compares the attributes of the new
  Llama 2 models with the Llama 1 models. 2.1 Pretraining Data Our
  training corpus includes a new mix of data from publicly available
  sources, which does not include data fromMeta’sproductsorservices.
  Wemadeanefforttoremovedatafromcertainsitesknowntocontaina
  highvolumeofpersonalinformationaboutprivateindividuals.
  Wetrainedon2trilliontokensofdataasthis
  providesagoodperformance–costtrade-off,up-samplingthemostfactualsourcesinanefforttoincrease
  knowledge and dampen hallucinations.
  Weperformedavarietyofpretrainingdatainvestigationssothatuserscanbetterunderstandthepotential
  capabilities and limitations of our models; results can be found in
  Section 4.1. 2.2 Training Details We adopt most of the pretraining
  setting and model architecture from Llama 1 .</p>
          </sec>
        </sec>
        <sec id="cell-1181af9d-680f-4ba3-89e2-f88b12a89cc7-nb-1" specific-use="notebook-code">
          <code language="python">from IPython.display import Markdown, display

response = subdoc_summary_pack.run(
    "What is the functionality of latest ChatGPT memory."
)
display(Markdown(str(response)))

for n in response.source_nodes:
    display_source_node(n, source_length=10000, metadata_mode="all")</code>
          <sec id="cell-1181af9d-680f-4ba3-89e2-f88b12a89cc7-output-0-nb-1" specific-use="notebook-output">
            <p>The latest ChatGPT model, equipped with Ghost Attention (GAtt),
  demonstrates strong multi-turn memory ability by consistently
  referring to defined attributes for up to 20 turns in a conversation.
  This integration of GAtt in the ChatGPT model allows for efficient
  long context attention beyond 2048 tokens, showcasing potential for
  robust performance in handling extended contexts.</p>
          </sec>
          <sec id="cell-1181af9d-680f-4ba3-89e2-f88b12a89cc7-output-1-nb-1" specific-use="notebook-output">
            <p><bold>Node ID:</bold>
  005a3c23-8d97-4e5d-957e-98ad2dfb93ad<bold>Similarity:</bold>
  0.7923889627946064<bold>Text:</bold> page_label: 54 file_name:
  llama2.pdf file_path: data/llama2.pdf file_type: application/pdf
  file_size: 13661300 creation_date: 2024-02-17 last_modified_date:
  2024-02-17 last_accessed_date: 2024-02-17 context_summary: Llama
  2-Chat with GAtt consistently refers to defined attributes for up to
  20 turns, showcasing strong multi-turn memory ability. The integration
  of GAtt in Llama 2-Chat enables efficient long context attention
  beyond 2048 tokens, indicating potential for robust performance in
  handling extended contexts.</p>
            <p>Dialogue Turn Baseline + GAtt 2 100% 100% 4 10% 100% 6 0% 100% 20
  0% 100% Table30: GAttresults. Llama 2-Chat
  withGAttisabletorefertoattributes100%ofthetime,forupto20 turns from
  our human evaluation. We limited the evaluated attributes to public
  figures and hobbies. Theattentionnowspansbeyond20turns.
  Wetestedthemodelabilitytorememberthesystemarguments
  troughahumanevaluation.
  Thearguments(e.g. hobbies,persona)aredefinedduringthefirstmessage,and
  then from turn 2 to 20. We explicitly asked the model to refer to them
  (e.g. “What is your favorite hobby?”,
  “Whatisyourname?”),tomeasurethemulti-turnmemoryabilityof Llama 2-Chat
  . Wereporttheresults inTable30. EquippedwithGAtt, Llama 2-Chat
  maintains100%accuracy,alwaysreferringtothedefined
  attribute,andso,upto20turns(wedidnotextendthehumanevaluationmore,andalltheexampleshad
  lessthan4048tokensintotalovertheturns). Asacomparison, Llama 2-Chat
  withoutGAttcannotanymore refer to the attributes after only few turns:
  from 100% at turn t+1, to 10% at turn t+3 and then 0%.
  GAttZero-shotGeneralisation.
  Wetriedatinferencetimetosetconstrainnotpresentinthetrainingof GAtt.
  For instance, “answer in one sentence only”, for which the model
  remained consistent, as illustrated in Figure 28. We applied first
  GAtt to Llama 1 , which was pretrained with a context length of 2048
  tokens and then fine-tuned with 4096 max length. We tested if GAtt
  works beyond 2048 tokens, and the model arguably managed to understand
  attributes beyond this window. This promising result indicates that
  GAtt could be adapted as an efficient technique for long context
  attention. A.3.6 How Far Can Model-Based Evaluation Go?</p>
          </sec>
          <sec id="cell-1181af9d-680f-4ba3-89e2-f88b12a89cc7-output-2-nb-1" specific-use="notebook-output">
            <p><bold>Node ID:</bold>
  0b1719e9-d7fa-42af-890b-5eeb946857c5<bold>Similarity:</bold>
  0.7837282816384877<bold>Text:</bold> page_label: 16 file_name:
  llama2.pdf file_path: data/llama2.pdf file_type: application/pdf
  file_size: 13661300 creation_date: 2024-02-17 last_modified_date:
  2024-02-17 last_accessed_date: 2024-02-17 context_summary: The text
  discusses the challenges faced in maintaining multi-turn consistency
  in dialogue systems and introduces a method called Ghost Attention
  (GAtt) to address these issues. GAtt involves incorporating
  instructions throughout a conversation to ensure dialogue control over
  multiple turns.</p>
            <p>Figure 9: Issues with multi-turn memory (left)can be improved with
  GAtt (right). We train for between 200and400iterations for all our
  models, and use evaluations on held-out prompts for earlystopping.
  EachiterationofPPOonthe70Bmodeltakesonaverage ≈330seconds.
  Totrainquicklywith large batch sizes, we use FSDP (Zhao et al., 2023).
  This was effective when using O(1) forward or backward
  passes,butcausedalargeslowdown(
  ≈20×)duringgeneration,evenwhenusingalargebatchsizeandKV cache. We were
  able to mitigate this by consolidating the model weights to each node
  once before generation and then freeing the memory after generation,
  resuming the rest of the training loop. 3.3 System Message for
  Multi-Turn Consistency In a dialogue setup, some instructions should
  apply for all the conversation turns, e.g., to respond succinctly, or
  to“act as”some public figure. When we provided such instructions to
  Llama 2-Chat , the subsequent response should always respect the
  constraint. However, our initial RLHF models tended to forget the
  initial instruction after a few turns of dialogue, as illustrated in
  Figure 9 (left). To address these limitations, we propose Ghost
  Attention (GAtt), a very simple method inspired by Context
  Distillation (Bai et al., 2022b) that hacks the fine-tuning data to
  help the attention focus in a multi-stage process. GAtt enables
  dialogue control over multiple turns, as illustrated in Figure 9
  (right). GAttMethod. Assumewe haveaccess toa multi-turndialoguedataset
  betweentwo persons(e.g., auser and an assistant), with a list of
  messages [u1, a1, . . . , u n, an], where unandancorrespond to the
  user and assistant messages for turn n, respectively. Then, we define
  an instruction, inst, that should be respected throughout the
  dialogue. For example, instcould be “act as.” We can then
  synthetically concatenate this instruction to all the user messages of
  the conversation. Next, we can sample from this synthetic data using
  the latest RLHF model.</p>
          </sec>
        </sec>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
</article>
